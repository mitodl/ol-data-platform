# - Process the raw_data_archive to extract the per-course assets
# - Model the different asset objects according to their type

import hashlib
import json
import re
import tarfile
import tempfile
from datetime import UTC, datetime
from pathlib import Path

import duckdb
import jsonlines
from dagster import (
    AssetExecutionContext,
    AssetIn,
    AssetKey,
    AssetMaterialization,
    AssetOut,
    AutoMaterializePolicy,
    Config,
    DagsterEventType,
    DataVersion,
    DefaultSensorStatus,
    DynamicOut,
    DynamicOutput,
    DynamicPartitionsDefinition,
    EventRecordsFilter,
    MetadataValue,
    MultiPartitionKey,
    OpExecutionContext,
    Output,
    RunRequest,
    SensorEvaluationContext,
    SensorResult,
    asset,
    multi_asset,
    op,
    sensor,
)
from dagster._core.definitions.data_version import DATA_VERSION_TAG
from flatten_dict import flatten
from flatten_dict.reducers import make_reducer
from google.cloud import storage
from pydantic import Field
from upath import UPath

from ol_orchestrate.lib.dagster_helpers import sanitize_mapping_key
from ol_orchestrate.lib.dagster_types.files import DagsterPath
from ol_orchestrate.lib.edxorg import (
    build_mapping_key,
    categorize_archive_element,
    parse_archive_path,
)
from ol_orchestrate.lib.openedx import un_nest_course_structure

edxorg_archive_partitions = DynamicPartitionsDefinition(name="edxorg_archive")
edxorg_tracking_log_partitions = DynamicPartitionsDefinition(name="edxorg_tracking_log")
course_and_source_partitions = DynamicPartitionsDefinition(name="course_and_source")
raw_archive_asset_key = AssetKey(("edxorg", "raw_data_archive"))
raw_tracking_log_asset_key = AssetKey(("edxorg", "raw_tracking_logs"))


@sensor(
    default_status=DefaultSensorStatus.STOPPED,
    required_resource_keys={"gcp_gcs"},
    minimum_interval_seconds=60 * 60,  # Set the tick frequency to hourly
)
def gcs_edxorg_archive_sensor(context: SensorEvaluationContext):
    dagster_instance = context.instance
    storage_client = context.resources.gcp_gcs.client
    bucket_name = "simeon-mitx-pipeline-main"
    bucket_prefix = "COLD"
    bucket_files: list[storage.Blob] = [
        file_
        for file_ in storage_client.list_blobs(bucket_name, prefix=bucket_prefix)
        if re.match(r"COLD/mitx-\d{4}-\d{2}-\d{2}.tar.gz$", file_.name)
        and file_.name.removeprefix("COLD/")
        not in edxorg_archive_partitions.get_partition_keys(
            dynamic_partitions_store=dagster_instance
        )
    ]

    assets = []
    partition_keys = []
    for file_ in sorted(bucket_files, key=lambda f: f.name):
        context.log.debug("Processing file %s", file_.name)
        partition_key = file_.name.removeprefix("COLD/")
        assets.append(
            AssetMaterialization(
                asset_key=raw_archive_asset_key,
                partition=partition_key,
                description=(
                    "Archive of data exported from edx.org for courses run by MIT. "
                    "Generated by https://github.com/openedx/edx-analytics-exporter/"
                ),
                metadata={
                    "source": "edxorg",
                    "path": MetadataValue.path(f"gs://{bucket_name}/{file_.name}"),
                    "creation_date": datetime.strptime(
                        re.search(r"(\d{4}-\d{2}-\d{2})", file_.name).groups()[0],  # type: ignore[union-attr]
                        "%Y-%m-%d",
                    )
                    .replace(tzinfo=UTC)
                    .strftime("%Y-%m-%d"),
                    "size (bytes)": file_.size,
                    "materialization_time": datetime.now(tz=UTC).isoformat(),
                },
                tags={DATA_VERSION_TAG: DataVersion(file_.etag).value},
            )
        )
        partition_keys.append(partition_key)

    return SensorResult(
        asset_events=assets,
        dynamic_partitions_requests=[
            edxorg_archive_partitions.build_add_request(partition_keys=partition_keys)
        ],
        run_requests=[
            RunRequest(
                job_name="retrieve_edx_course_exports",
                run_key=partition,
                partition_key=partition,
            )
            for partition in partition_keys
        ],
    )


@asset(
    partitions_def=edxorg_archive_partitions,
    io_manager_key="gcs_input",
    key=raw_archive_asset_key,
    group_name="edxorg",
)
def edxorg_raw_data_archive(): ...


class EdxorgArchiveProcessConfig(Config):
    s3_bucket: str = Field(description="S3 bucket to push processed files to.")
    s3_prefix: str = Field(
        default="", description="S3 object key prefix to push processed files to."
    )


@op(
    name="process_edxorg_archive_bundle",
    required_resource_keys={"gcp_gcs"},
    out={
        "course_structure": DynamicOut(is_required=False),
        "course_xml": DynamicOut(is_required=False),
        "forum_mongo": DynamicOut(is_required=False),
        "db_table__assessment_assessment": DynamicOut(is_required=False),
        "db_table__assessment_assessmentfeedback": DynamicOut(is_required=False),
        "db_table__assessment_assessmentfeedback_assessments": DynamicOut(
            is_required=False
        ),
        "db_table__assessment_assessmentfeedback_options": DynamicOut(
            is_required=False
        ),
        "db_table__assessment_assessmentfeedbackoption": DynamicOut(is_required=False),
        "db_table__assessment_assessmentpart": DynamicOut(is_required=False),
        "db_table__assessment_criterion": DynamicOut(is_required=False),
        "db_table__assessment_criterionoption": DynamicOut(is_required=False),
        "db_table__assessment_peerworkflow": DynamicOut(is_required=False),
        "db_table__assessment_peerworkflowitem": DynamicOut(is_required=False),
        "db_table__assessment_rubric": DynamicOut(is_required=False),
        "db_table__assessment_studenttrainingworkflow": DynamicOut(is_required=False),
        "db_table__assessment_studenttrainingworkflowitem": DynamicOut(
            is_required=False
        ),
        "db_table__assessment_trainingexample": DynamicOut(is_required=False),
        "db_table__assessment_trainingexample_options_selected": DynamicOut(
            is_required=False
        ),
        "db_table__auth_user": DynamicOut(is_required=False),
        "db_table__auth_userprofile": DynamicOut(is_required=False),
        "db_table__certificates_generatedcertificate": DynamicOut(is_required=False),
        "db_table__course": DynamicOut(is_required=False),
        "db_table__course_groups_cohortmembership": DynamicOut(is_required=False),
        "db_table__course_structure": DynamicOut(is_required=False),
        "db_table__courseware_studentmodule": DynamicOut(is_required=False),
        "db_table__credit_crediteligibility": DynamicOut(is_required=False),
        "db_table__django_comment_client_role_users": DynamicOut(is_required=False),
        "db_table__examples": DynamicOut(is_required=False),
        "db_table__grades_persistentcoursegrade": DynamicOut(is_required=False),
        "db_table__grades_persistentsubsectiongrade": DynamicOut(is_required=False),
        "db_table__student_anonymoususerid": DynamicOut(is_required=False),
        "db_table__student_courseaccessrole": DynamicOut(is_required=False),
        "db_table__student_courseenrollment": DynamicOut(is_required=False),
        "db_table__student_languageproficiency": DynamicOut(is_required=False),
        "db_table__submissions_score": DynamicOut(is_required=False),
        "db_table__submissions_scoresummary": DynamicOut(is_required=False),
        "db_table__submissions_studentitem": DynamicOut(is_required=False),
        "db_table__submissions_submission": DynamicOut(is_required=False),
        "db_table__teams": DynamicOut(is_required=False),
        "db_table__teams_membership": DynamicOut(is_required=False),
        "db_table__user_api_usercoursetag": DynamicOut(is_required=False),
        "db_table__user_id_map": DynamicOut(is_required=False),
        "db_table__validate": DynamicOut(is_required=False),
        "db_table__wiki_article": DynamicOut(is_required=False),
        "db_table__wiki_articlerevision": DynamicOut(is_required=False),
        "db_table__workflow_assessmentworkflow": DynamicOut(is_required=False),
        "db_table__workflow_assessmentworkflowstep": DynamicOut(is_required=False),
    },
)
def process_edxorg_archive_bundle(
    context: OpExecutionContext,
    config: EdxorgArchiveProcessConfig,
    edxorg_raw_data_archive: DagsterPath,
):
    archive = tarfile.open(edxorg_raw_data_archive)
    dagster_instance = context.instance
    input_asset_materialization_event = dagster_instance.get_event_records(
        event_records_filter=EventRecordsFilter(
            asset_key=context.asset_key_for_input("edxorg_raw_data_archive"),
            event_type=DagsterEventType.ASSET_MATERIALIZATION,
            asset_partitions=[context.partition_key],
        ),
        limit=1,
    )[0]

    tmpdir = tempfile.TemporaryDirectory()
    while tinfo := archive.next():
        if not tinfo.isdir():
            context.log.debug("Processing archive path %s", tinfo.name)
            asset_info = parse_archive_path(tinfo.name)
            if not asset_info:
                continue
            normalized_source_system = (
                "edge" if "edge" in asset_info["source_system"] else "prod"
            )
            dagster_instance.add_dynamic_partitions(
                course_and_source_partitions.name,
                partition_keys=[
                    MultiPartitionKey(
                        {
                            "course_id": asset_info["course_id"],
                            "source_system": normalized_source_system,
                        }
                    )
                ],
            )
            normalized_extension = (
                "tsv" if asset_info["extension"] == "sql" else asset_info["extension"]
            )
            if table_name := asset_info.get("table_name"):
                output_key = f"db_table__{table_name}"
            else:
                output_key = categorize_archive_element(tinfo.name)
            archive_file = Path(tmpdir.name).joinpath(tinfo.name.split("/")[-1])
            archive_file.write_bytes(archive.extractfile(tinfo).read())  # type: ignore[union-attr]
            data_version = hashlib.file_digest(
                archive_file.open("rb"), "sha256"
            ).hexdigest()
            mapping_key = build_mapping_key(asset_info)
            context.log.debug(
                "Edxorg output asset mapping key for asset info %s: %s",
                asset_info,
                mapping_key,
            )
            shared_metadata = {
                "path": MetadataValue.path(
                    f"s3://{config.s3_bucket}/{config.s3_prefix}/{'/'.join(mapping_key)}/{data_version}.{normalized_extension}"
                ),
                "object_key": f"{'/'.join(mapping_key)}/{data_version}.{normalized_extension}",  # noqa: E501
                "source": "edxorg",
                "source_system": normalized_source_system,
                "course_id": asset_info["course_id"],
            }
            yield DynamicOutput(
                (
                    archive_file,
                    f"s3://{config.s3_bucket}/{config.s3_prefix}/{'/'.join(mapping_key)}/{data_version}.{normalized_extension}",
                ),
                output_name=output_key,
                mapping_key=sanitize_mapping_key("/".join(mapping_key)),
                metadata=shared_metadata,
            )
            materialization = AssetMaterialization(
                asset_key=AssetKey(build_mapping_key(asset_info, is_asset_key=True)),
                partition=MultiPartitionKey(
                    {
                        "course_id": asset_info["course_id"],
                        "source_system": normalized_source_system,
                    }
                ),
                metadata=shared_metadata,
                tags={
                    DATA_VERSION_TAG: data_version,
                    "dagster/input_event_pointer/edxorg/raw_data_archive": str(
                        input_asset_materialization_event.storage_id
                    ),
                    "dagster/input_data_version/edxorg/raw_data_archive": input_asset_materialization_event.asset_materialization.tags.get(  # noqa: E501
                        DATA_VERSION_TAG
                    ),
                },
            )
            yield materialization
    # Clean up the downloaded archive file so that it doesn't consume the local disk
    edxorg_raw_data_archive.unlink()


@asset(
    key=AssetKey(("edxorg", "raw_data", "course_structure")),
    partitions_def=course_and_source_partitions,
    group_name="edxorg",
)
def dummy_edxorg_course_structure(): ...


@multi_asset(
    outs={
        "flattened_course_structure": AssetOut(
            key=AssetKey(("edxorg", "processed_data", "flattened_course_structure")),
            io_manager_key="s3file_io_manager",
            description=(
                "A flattened representation of the structure information "
                "for a given course, with one row per course."
            ),
            auto_materialize_policy=AutoMaterializePolicy.eager(
                max_materializations_per_minute=None
            ),
        ),
        "course_blocks": AssetOut(
            key=AssetKey(("edxorg", "processed_data", "course_blocks")),
            io_manager_key="s3file_io_manager",
            description=(
                "A hierarchical representation of the structure information "
                "for a given course, with one row per course block."
            ),
            auto_materialize_policy=AutoMaterializePolicy.eager(
                max_materializations_per_minute=None
            ),
        ),
    },
    ins={
        "course_structure": AssetIn(
            key=AssetKey(("edxorg", "raw_data", "course_structure"))
        )
    },
    partitions_def=course_and_source_partitions,
    group_name="edxorg",
)
def flatten_edxorg_course_structure(
    context: AssetExecutionContext, course_structure: UPath
):
    dagster_instance = context.instance
    input_asset_materialization_event = dagster_instance.get_event_records(
        event_records_filter=EventRecordsFilter(
            asset_key=context.asset_key_for_input("course_structure"),
            event_type=DagsterEventType.ASSET_MATERIALIZATION,
            asset_partitions=[context.partition_key],
        ),
        limit=1,
    )[0]
    course_id = input_asset_materialization_event.asset_materialization.metadata[
        "course_id"
    ]
    course_structure_document = json.load(course_structure.open())
    data_version = hashlib.sha256(
        json.dumps(course_structure_document).encode("utf-8")
    ).hexdigest()
    structures_file = Path(f"course_structures_{data_version}.json")
    blocks_file = Path(f"course_blocks_{data_version}.json")
    data_retrieval_timestamp = datetime.now(tz=UTC).isoformat()
    with (
        jsonlines.open(structures_file, mode="w") as structures,
        jsonlines.open(blocks_file, mode="w") as blocks,
    ):
        table_row = {
            "content_hash": hashlib.sha256(
                json.dumps(course_structure_document).encode("utf-8")
            ).hexdigest(),
            "course_id": context.partition_key,
            "course_structure": course_structure_document,
            "course_structure_flattened": flatten(
                course_structure_document,
                reducer=make_reducer("__"),
            ),
            "retrieved_at": data_retrieval_timestamp,
        }
        structures.write(table_row)
        for block in un_nest_course_structure(
            course_id, course_structure_document, data_retrieval_timestamp
        ):
            blocks.write(block)
    flattened_structure_object_key = f"edxorg/processed_data/flattened_course_structure/{context.partition_key}/{data_version}.json"  # noqa: E501
    blocks_object_key = f"edxorg/processed_data/course_blocks/{context.partition_key}/{data_version}.json"  # noqa: E501
    yield Output(
        (structures_file, flattened_structure_object_key),
        output_name="flattened_course_structure",
        data_version=DataVersion(data_version),
        metadata={"course_id": course_id, "object_key": flattened_structure_object_key},
    )
    yield Output(
        (blocks_file, blocks_object_key),
        output_name="course_blocks",
        data_version=DataVersion(data_version),
        metadata={
            "course_id": course_id,
            "object_key": blocks_object_key,
        },
    )


#####################################
# Manage Tracking Logs From edx.org #
#####################################


@sensor(
    default_status=DefaultSensorStatus.STOPPED,
    required_resource_keys={"gcp_gcs"},
    minimum_interval_seconds=60 * 60,  # Set the tick frequency to hourly
)
def gcs_edxorg_tracking_log_sensor(context: SensorEvaluationContext):
    dagster_instance = context.instance
    storage_client = context.resources.gcp_gcs.client
    bucket_name = "simeon-mitx-pipeline-main"
    bucket_prefix = "COLD"
    bucket_files: list[storage.Blob] = [
        file_
        for file_ in storage_client.list_blobs(bucket_name, prefix=bucket_prefix)
        if re.match(r"COLD/mitx-edx-events-\d{4}-\d{2}-\d{2}.log.gz$", file_.name)
        and file_.name.removeprefix("COLD/")
        not in edxorg_tracking_log_partitions.get_partition_keys(
            dynamic_partitions_store=dagster_instance
        )
    ]

    assets = []
    partition_keys = []
    for file_ in sorted(bucket_files, key=lambda f: f.name):
        context.log.debug("Processing file %s", file_.name)
        partition_key = file_.name.removeprefix("COLD/")
        assets.append(
            AssetMaterialization(
                asset_key=raw_tracking_log_asset_key,
                partition=partition_key,
                description=(
                    "User interaction event logs generated from courses hosted on "
                    "edx.org"
                ),
                metadata={
                    "source": "edxorg",
                    "path": MetadataValue.path(f"gs://{bucket_name}/{file_.name}"),
                    "creation_date": datetime.strptime(
                        re.search(r"(\d{4}-\d{2}-\d{2})", file_.name).groups()[0],  # type: ignore[union-attr]
                        "%Y-%m-%d",
                    )
                    .replace(tzinfo=UTC)
                    .strftime("%Y-%m-%d"),
                    "size (bytes)": file_.size,
                    "materialization_time": datetime.now(tz=UTC).isoformat(),
                },
                tags={DATA_VERSION_TAG: DataVersion(file_.etag).value},
            )
        )
        partition_keys.append(partition_key)

    return SensorResult(
        asset_events=assets,
        dynamic_partitions_requests=[
            edxorg_tracking_log_partitions.build_add_request(
                partition_keys=partition_keys
            )
        ],
    )


@asset(
    partitions_def=edxorg_tracking_log_partitions,
    io_manager_key="gcs_input",
    key=raw_tracking_log_asset_key,
    group_name="edxorg",
)
def edxorg_raw_tracking_logs(): ...


@asset(
    partitions_def=edxorg_tracking_log_partitions,
    key=AssetKey(("edxorg", "raw_data", "tracking_logs")),
    group_name="edxorg",
    io_manager_key="s3file_io_manager",
    ins={
        "edxorg_raw_tracking_log": AssetIn(
            key=raw_tracking_log_asset_key,
        )
    },
    auto_materialize_policy=AutoMaterializePolicy.eager(
        max_materializations_per_minute=None
    ),
)
def normalize_edxorg_tracking_log(
    context: AssetExecutionContext, edxorg_raw_tracking_log: DagsterPath
):
    database_path = f"{context.run_id}_edxorg_tracking_logs.db"
    transformed_logs = Path(f"normalized_{context.partition_key}")
    with duckdb.connect(database=database_path) as conn:
        conn.execute("DROP TABLE IF EXISTS tracking_logs")
        conn.execute("INSTALL json;")
        conn.execute(
            f"""
            CREATE TABLE tracking_logs AS
            SELECT * FROM read_ndjson_auto('{edxorg_raw_tracking_log}',
            FILENAME=1, union_by_name=1, maximum_depth=1);
            """  # noqa: S608
        )
        col_names = conn.execute(
            """SELECT column_name FROM temp.information_schema.columns
            WHERE table_name = 'tracking_logs'
            """
        ).fetchall()
        # exclude filename, which is already a VARCHAR
        columns = [i[0] for i in col_names]
        columns.remove("filename")
        update_stmts = []
        for col in columns:
            conn.execute(f"ALTER TABLE tracking_logs ALTER {col} TYPE VARCHAR")
            update_stmts.append(f"{col} = json_extract_string({col}, '$')")
        # extract VARCHAR strings from json for every field
        update_query = f"UPDATE tracking_logs SET {', '.join(update_stmts)}"  # noqa: S608
        conn.execute(update_query)
        # convert integer timestamps to datetime
        conn.execute(
            """UPDATE tracking_logs
            SET time = to_timestamp(CAST(time AS BIGINT))
            WHERE TRY_CAST(time AS BIGINT) IS NOT NULL
            """
        )
        # convert all timestamps to iso8601 format
        conn.execute(
            """
            UPDATE tracking_logs
            SET time = strftime(TRY_CAST(time AS TIMESTAMP), '%Y-%m-%d %H:%M:%S.%f')
            """
        )
        conn.execute(
            f"""COPY (SELECT {', '.join(columns)} FROM tracking_logs)
            TO '{transformed_logs}' (FORMAT JSON)
            """  # noqa: S608
        )
        s3_object_key = f"edxorg/raw_data/tracking_logs/{transformed_logs}"

        yield Output(
            (transformed_logs, s3_object_key),
            metadata={
                "object_key": s3_object_key,
                "source": "edxorg",
            },
        )
    # Clean up the processed tracking log so it doesn't use up the local disk
    edxorg_raw_tracking_log.unlink()
    Path(database_path).unlink()
