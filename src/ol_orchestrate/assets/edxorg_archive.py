# - Process the raw_data_archive to extract the per-course assets
# - Model the different asset objects according to their type

import hashlib
import re
import tarfile
from datetime import UTC, datetime
from pathlib import Path

from dagster import (
    AssetKey,
    AssetMaterialization,
    Config,
    DataVersion,
    DefaultSensorStatus,
    DynamicOut,
    DynamicOutput,
    DynamicPartitionsDefinition,
    In,
    MetadataValue,
    OpExecutionContext,
    RunRequest,
    SensorEvaluationContext,
    SensorResult,
    op,
    sensor,
)
from pydantic import Field
from upath import UPath

from ol_orchestrate.lib.edxorg import (
    build_mapping_key,
    categorize_archive_element,
    parse_archive_path,
)

edxorg_archive_partitions = DynamicPartitionsDefinition(name="edxorg_archive")
course_and_source_partitions = DynamicPartitionsDefinition(name="course_and_source")


@sensor(
    default_status=DefaultSensorStatus.RUNNING,
    required_resource_keys={"gcp_gcs"},
)
def gcs_edxorg_archive_sensor(context: SensorEvaluationContext):
    dagster_instance = context.instance
    storage_client = context.resources.gcp_gcs.client
    bucket_name = "simeon-mitx-pipeline-main"
    bucket_prefix = "COLD"
    bucket_files = {
        file_
        for file_ in storage_client.list_blobs(bucket_name, prefix=bucket_prefix)
        if re.match(r"COLD/mitx-\d{4}-\d{2}-\d{2}.tar.gz$", file_.name)
        and file_.name.removeprefix("COLD/")
        not in edxorg_archive_partitions.get_partition_keys(
            dynamic_partitions_store=dagster_instance
        )
    }

    assets = []
    partition_keys = []
    for file_ in bucket_files:
        context.log.debug("Processing file %s", file_.name)
        assets.append(
            AssetMaterialization(
                asset_key=AssetKey(("edxorg", "raw_data_archive")),
                partition=file_.name.removeprefix("COLD/"),
                description=(
                    "Archive of data exported from edx.org for courses run by MIT. "
                    "Generated by https://github.com/openedx/edx-analytics-exporter/"
                ),
                metadata={
                    "source": "edxorg",
                    "path": MetadataValue.path(f"gs://{bucket_name}/{file_.name}"),
                    "creation_date": datetime.strptime(
                        re.search(r"(\d{4}-\d{2}-\d{2})", file_.name).groups()[0],
                        "%Y-%m-%d",
                    ).strftime("%Y-%m-%d"),
                    "size (bytes)": file_.size,
                    "materialization_time": datetime.now(tz=UTC).isoformat(),
                },
            )
        )
        partition_keys.append(file_.name.removeprefix("COLD/"))

    return SensorResult(
        asset_events=assets,
        dynamic_partitions_requests=[
            edxorg_archive_partitions.build_add_request(partition_keys=partition_keys)
        ],
        run_requests=[
            RunRequest(
                job_name="retrieve_edx_course_exports",
                run_key=partition,
                partition_key=partition,
            )
            for partition in partition_keys
        ],
    )


class EdxorgArchiveProcessConfig(Config):
    s3_bucket: str = Field(description="S3 bucket to push processed files to.")
    s3_prefix: str = Field(
        default="", description="S3 object key prefix to push processed files to."
    )


@op(
    name="process_edxorg_archive_bundle",
    required_resource_keys={"gcp_gcs"},
    out=DynamicOut(),
    ins={
        "edxorg_raw_data_archive": In(
            asset_key=AssetKey(("edxorg", "raw_data_archive")),
            input_manager_key="gcs_input",
        )
    },
)
def process_edxorg_archive_bundle(
    context: OpExecutionContext,
    config: EdxorgArchiveProcessConfig,
    edxorg_raw_data_archive: UPath,
):
    archive = tarfile.open(edxorg_raw_data_archive)
    dagster_instance = context.instance
    while tinfo := archive.next():
        if not tinfo.isdir():
            context.log.debug("Processing archive path %s", tinfo.name)
            asset_info = parse_archive_path(tinfo.name)
            if not asset_info:
                continue
            normalized_source_system = (
                "edge" if "edge" in asset_info["source_system"] else "prod"
            )
            dagster_instance.add_dynamic_partitions(
                course_and_source_partitions.name,
                partition_keys=[normalized_source_system, asset_info["course_id"]],
            )
            normalized_extension = (
                "tsv" if asset_info["extension"] == "sql" else asset_info["extension"]
            )
            output_key = asset_info.get("table_name") or categorize_archive_element(
                tinfo.name
            )
            archive_file = Path(tinfo.name.split("/")[-1])
            archive_file.write_bytes(archive.extractfile(tinfo).read())
            data_version = hashlib.file_digest(
                archive_file.open("rb"), "sha256"
            ).hexdigest()
            mapping_key = build_mapping_key(asset_info)
            shared_metadata = {
                "path": MetadataValue.path(
                    f"s3://{config.s3_bucket}/{config.s3_prefix}/{mapping_key}/{data_version}.{normalized_extension}"
                ),
                "object_key": f"{mapping_key}/{data_version}.{normalized_extension}",
                "source": "edxorg",
                "source_system": normalized_source_system,
                "course_id": asset_info["course_id"],
            }
            yield DynamicOutput(
                archive_file,
                output_name=output_key,
                data_version=DataVersion(data_version),
                mapping_key=mapping_key,
                metadata=shared_metadata,
            )
            yield AssetMaterialization(
                asset_key=AssetKey(build_mapping_key(asset_info, is_asset_key=True)),
                partition=[normalized_source_system, asset_info["course_id"]],
                metadata=shared_metadata,
            )
            archive_file.unlink()
