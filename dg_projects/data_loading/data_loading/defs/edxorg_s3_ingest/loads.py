"""
EdX.org S3 CSV/TSV data ingestion via dlt.

This module loads TSV files from S3 that are generated by the edxorg_archive.py
asset. These files contain database table exports from edX.org courses.

S3 Structure:
s3://ol-data-lake-landing-zone-production/edxorg-raw-data/edxorg/raw_data/db_table/{table_name}/{source_system}/{course_id}/{hash}.tsv

Tables include:
- auth_user, auth_userprofile
- student_courseenrollment, courseware_studentmodule
- certificates_generatedcertificate
- submissions_*, assessment_*
- And many more...

Usage (standalone):
    # Local development (default - writes to .dlt/data/)
    python -m data_loading.defs.edxorg_s3_ingest.loads

    # Production (writes to S3)
    DAGSTER_ENVIRONMENT=production python -m data_loading.defs.edxorg_s3_ingest.loads
"""

import logging
import os
from collections.abc import Iterator
from pathlib import Path

import dlt
from dlt.sources import incremental
from dlt.sources.filesystem import filesystem, read_csv_duckdb

logger = logging.getLogger(__name__)

# Set dlt project directory to the data_loading project root
# This ensures .dlt/config.toml is found when running from repo root
DLT_PROJECT_DIR = Path(__file__).parent.parent.parent.parent
if DLT_PROJECT_DIR.exists():
    os.environ.setdefault("DLT_PROJECT_DIR", str(DLT_PROJECT_DIR))


@dlt.source
def edxorg_s3_source(
    bucket_url: str = dlt.config.value,
    tables: list[str] | None = None,
    table_format: str = "parquet",  # "parquet" for local, "iceberg" for QA/prod
):
    """
    Load edxorg CSV/TSV data from S3.

    Uses AWS IAM authentication automatically:
    - Local development: Uses ~/.aws/credentials
    - Production/K8s: Uses IRSA (IAM Roles for Service Accounts)

    Args:
        bucket_url: S3 bucket URL containing TSV files
        tables: Optional list of specific tables to load (loads all if None)
        table_format: "parquet" for local dev, "iceberg" for QA/production

    Yields:
        dlt resources for each table type found in S3
    """

    # Available table names from edxorg_archive.py
    all_tables = [
        "assessment_assessment",
        "assessment_assessmentfeedback",
        "assessment_assessmentfeedback_assessments",
        "assessment_assessmentfeedback_options",
        "assessment_assessmentfeedbackoption",
        "assessment_assessmentpart",
        "assessment_criterion",
        "assessment_criterionoption",
        "assessment_peerworkflow",
        "assessment_peerworkflowitem",
        "assessment_rubric",
        "assessment_studenttrainingworkflow",
        "assessment_studenttrainingworkflowitem",
        "assessment_trainingexample",
        "assessment_trainingexample_options_selected",
        "auth_user",
        "auth_userprofile",
        "certificates_generatedcertificate",
        "course",
        "course_groups_cohortmembership",
        "courseware_studentmodule",
        "credit_crediteligibility",
        "django_comment_client_role_users",
        "grades_persistentcoursegrade",
        "grades_persistentsubsectiongrade",
        "student_anonymoususerid",
        "student_courseaccessrole",
        "student_courseenrollment",
        "student_languageproficiency",
        "submissions_score",
        "submissions_scoresummary",
        "submissions_studentitem",
        "submissions_submission",
        "teams",
        "teams_membership",
        "user_api_usercoursetag",
        "user_id_map",
        "wiki_article",
        "wiki_articlerevision",
        "workflow_assessmentworkflow",
        "workflow_assessmentworkflowstep",
    ]

    # Filter to requested tables if specified
    tables_to_load = tables if tables is not None else all_tables

    for table_name in tables_to_load:
        # Create a resource for each table with standardized naming
        @dlt.resource(
            name=f"raw__edxorg__s3__tables__{table_name}",
            write_disposition="append",
            primary_key=None,  # TSV files don't have consistent primary keys
            table_format=table_format,  # Use iceberg for QA/prod, parquet for local
        )
        def load_table(table=table_name) -> Iterator[dict[str, str]]:
            """
            Load TSV files for a specific table from S3.

            Uses incremental loading based on file modification_date to avoid
            reprocessing unchanged files on subsequent runs.

            Only processes 'prod' source files, excluding 'edge' staging environment.
            """

            # Pattern to match ONLY prod TSV files for this table
            # Matches: db_table/{table_name}/prod/*/*.tsv (prod/course_id/*.tsv)
            # Excludes: db_table/{table_name}/edge/*/*.tsv (edge staging files)
            file_glob = f"db_table/{table}/prod/**/*.tsv"

            # Use dlt's filesystem source to discover files
            # AWS credentials will be automatically discovered from:
            # 1. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
            # 2. ~/.aws/credentials (local development)
            # 3. IAM role (IRSA in Kubernetes)
            files = filesystem(
                bucket_url=bucket_url,
                file_glob=file_glob,
                extract_content=True,
            )

            # Enable incremental loading based on file modification date
            # Only processes files modified since the last successful run
            files.apply_hints(incremental=incremental("modification_date"))

            return files | read_csv_duckdb(
                use_pyarrow=True,
                delimiter="\t",  # TSV files use tabs
                ignore_errors=True,
            )

        yield load_table


# Determine environment and destination
# Use DAGSTER_ENVIRONMENT to determine destination and Glue database
dagster_env = os.getenv("DAGSTER_ENVIRONMENT", "dev")

# Map DAGSTER_ENVIRONMENT to destination config
# dev/ci -> local filesystem (Parquet, no Iceberg, no Glue)
# qa -> filesystem + Iceberg (S3 + ol_warehouse_qa_raw Glue database)
# production -> filesystem + Iceberg (S3 + ol_warehouse_production_raw Glue database)
table_format = "iceberg"
if dagster_env in ("qa", "production"):
    destination_env = dagster_env
    dataset_name = f"ol_warehouse_{dagster_env}_raw"
else:
    # dev, ci, or any other value uses local
    destination_env = "local"
    dataset_name = "edxorg_s3_local"

# Create source instance with appropriate table format
# dlt resolves config from .dlt/config.toml at runtime
edxorg_s3_source_instance = edxorg_s3_source(
    table_format=table_format,
)

# Rename config sections to match dlt's named destination pattern
# For named destinations, dlt looks for [destination.<name>] with destination_type set

# Update config.toml structure:
# - [destination.local] with destination_type="filesystem"
# - [destination.qa] with destination_type="filesystem"
# - [destination.production] with destination_type="filesystem"

# Create pipeline with environment-specific named destination
edxorg_s3_pipeline = dlt.pipeline(
    pipeline_name="edxorg_s3",
    destination=destination_env,  # References [destination.<env>] in config.toml
    dataset_name=dataset_name,
    progress="log",
)


def run_pipeline(tables: list[str] | None = None):
    """
    Execute the edxorg S3 pipeline.

    Args:
        tables: Optional list of specific tables to load.
                If None, loads all tables (can be slow!).
                Example: ["auth_user", "student_courseenrollment"]
    """
    logger.info("Running edxorg S3 pipeline with destination: %s", destination_env)

    if tables:
        logger.info("Loading specific tables: %s", ", ".join(tables))
    else:
        logger.info("Loading all available tables (this may take a while)")

    load_info = edxorg_s3_pipeline.run(
        edxorg_s3_source(tables=tables),
        loader_file_format="parquet",
    )
    logger.info("Pipeline completed: %s", load_info)

    return load_info


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    # For testing, only load a small subset of tables
    test_tables = ["auth_user", "student_courseenrollment"]

    logger.info("Running with test tables for faster execution.")
    logger.info("To load all tables, call run_pipeline(tables=None)")

    run_pipeline(tables=test_tables)
