"""
EdX.org S3 CSV/TSV data ingestion via dlt.

This module loads TSV files from S3 that are generated by the edxorg_archive.py
asset. These files contain database table exports from edX.org courses.

S3 Structure:
s3://ol-data-lake-landing-zone-production/edxorg-raw-data/edxorg/raw_data/db_table/{table_name}/{source_system}/{course_id}/{hash}.tsv

Tables include:
- auth_user, auth_userprofile
- student_courseenrollment, courseware_studentmodule
- certificates_generatedcertificate
- submissions_*, assessment_*
- And many more...

Usage (standalone):
    # Local development (default - writes to .dlt/data/)
    python -m lakehouse.defs.edxorg_s3_ingestion.loads

    # Production (writes to S3)
    DLT_DESTINATION_ENV=production python -m lakehouse.defs.edxorg_s3_ingestion.loads
"""

import logging
import os
from collections.abc import Iterator
from pathlib import Path

import dlt
from dlt.sources import incremental
from dlt.sources.filesystem import filesystem, read_csv

logger = logging.getLogger(__name__)

# Set dlt project directory to the data_loading project root
# This ensures .dlt/config.toml is found when running from repo root
DLT_PROJECT_DIR = Path(__file__).parent.parent.parent.parent
if DLT_PROJECT_DIR.exists():
    os.environ.setdefault("DLT_PROJECT_DIR", str(DLT_PROJECT_DIR))


@dlt.source
def edxorg_s3_source(
    bucket_url: str = dlt.config.value,
    tables: list[str] | None = None,
    table_format: str = "parquet",  # "parquet" for local, "iceberg" for QA/prod
):
    """
    Load edxorg CSV/TSV data from S3.

    Uses AWS IAM authentication automatically:
    - Local development: Uses ~/.aws/credentials
    - Production/K8s: Uses IRSA (IAM Roles for Service Accounts)

    Args:
        bucket_url: S3 bucket URL containing TSV files
        tables: Optional list of specific tables to load (loads all if None)
        table_format: "parquet" for local dev, "iceberg" for QA/production

    Yields:
        dlt resources for each table type found in S3
    """

    # Available table names from edxorg_archive.py
    all_tables = [
        "assessment_assessment",
        "assessment_assessmentfeedback",
        "assessment_assessmentfeedback_assessments",
        "assessment_assessmentfeedback_options",
        "assessment_assessmentfeedbackoption",
        "assessment_assessmentpart",
        "assessment_criterion",
        "assessment_criterionoption",
        "assessment_peerworkflow",
        "assessment_peerworkflowitem",
        "assessment_rubric",
        "assessment_studenttrainingworkflow",
        "assessment_studenttrainingworkflowitem",
        "assessment_trainingexample",
        "assessment_trainingexample_options_selected",
        "auth_user",
        "auth_userprofile",
        "certificates_generatedcertificate",
        "course_groups_cohortmembership",
        "courseware_studentmodule",
        "credit_crediteligibility",
        "django_comment_client_role_users",
        "grades_persistentcoursegrade",
        "grades_persistentsubsectiongrade",
        "student_anonymoususerid",
        "student_courseaccessrole",
        "student_courseenrollment",
        "student_languageproficiency",
        "submissions_score",
        "submissions_scoresummary",
        "submissions_studentitem",
        "submissions_submission",
        "teams",
        "teams_membership",
        "user_api_usercoursetag",
        "user_id_map",
        "wiki_article",
        "wiki_articlerevision",
        "workflow_assessmentworkflow",
        "workflow_assessmentworkflowstep",
    ]

    # Filter to requested tables if specified
    tables_to_load = tables if tables is not None else all_tables

    for table_name in tables_to_load:
        # Create a resource for each table with standardized naming
        @dlt.resource(
            name=f"raw__edxorg__s3__tables__{table_name}",
            write_disposition="merge",
            primary_key=None,  # TSV files don't have consistent primary keys
            table_format=table_format,  # Use iceberg for QA/prod, parquet for local
        )
        def load_table(table=table_name) -> Iterator[dict[str, str]]:
            """
            Load TSV files for a specific table from S3.

            Uses incremental loading based on file modification_date to avoid
            reprocessing unchanged files on subsequent runs.

            Only processes 'prod' source files, excluding 'edge' staging environment.
            """

            # Pattern to match ONLY prod TSV files for this table
            # Matches: db_table/{table_name}/prod/*/*.tsv (prod/course_id/*.tsv)
            # Excludes: db_table/{table_name}/edge/*/*.tsv (edge staging files)
            file_glob = f"db_table/{table}/prod/**/*.tsv"

            # Use dlt's filesystem source to discover files
            # AWS credentials will be automatically discovered from:
            # 1. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
            # 2. ~/.aws/credentials (local development)
            # 3. IAM role (IRSA in Kubernetes)
            files = filesystem(
                bucket_url=bucket_url,
                file_glob=file_glob,
            )

            # Enable incremental loading based on file modification date
            # Only processes files modified since the last successful run
            files.apply_hints(incremental=incremental("modification_date"))

            # Read and yield data from each TSV file
            for file_item in files:
                # Read TSV with appropriate settings
                csv_reader = read_csv(
                    file_item,
                    delimiter="\t",  # TSV files use tabs
                    chunksize=1000,  # Process in chunks for memory efficiency
                )

                for chunk in csv_reader:
                    # Convert chunk to records and yield
                    for record in chunk.to_dict(orient="records"):
                        # Add metadata about source file
                        record["_dlt_file_path"] = file_item["file_name"]
                        record["_dlt_load_timestamp"] = dlt.current.load_id()
                        # Include modification date for tracking
                        record["_dlt_file_modified"] = file_item.get(
                            "modification_date"
                        )
                        yield record

        yield load_table


# Determine environment and destination
# Use DAGSTER_ENVIRONMENT to determine destination and Glue database
dagster_env = os.getenv("DAGSTER_ENVIRONMENT", "dev")

# Map DAGSTER_ENVIRONMENT to destination config
# dev/ci -> local filesystem (Parquet, no Iceberg, no Glue)
# qa -> filesystem + Iceberg (S3 + ol_warehouse_qa_raw Glue database)
# production -> filesystem + Iceberg (S3 + ol_warehouse_production_raw Glue database)
if dagster_env in ("qa", "production"):
    destination_env = dagster_env
    dataset_name = f"ol_warehouse_{dagster_env}_raw"
    table_format = "iceberg"
else:
    # dev, ci, or any other value uses local
    destination_env = "local"
    dataset_name = "edxorg_s3_local"
    table_format = "parquet"

# Create source instance with appropriate table format
# dlt resolves config from .dlt/config.toml at runtime
edxorg_s3_source_instance = edxorg_s3_source(table_format=table_format)

# Create pipeline with environment-specific configuration
edxorg_s3_pipeline = dlt.pipeline(
    pipeline_name="edxorg_s3",
    destination="filesystem",
    dataset_name=dataset_name,
    progress="log",
)


def run_pipeline(tables: list[str] | None = None):
    """
    Execute the edxorg S3 pipeline.

    Args:
        tables: Optional list of specific tables to load.
                If None, loads all tables (can be slow!).
                Example: ["auth_user", "student_courseenrollment"]
    """
    logger.info("Running edxorg S3 pipeline with destination: %s", destination_env)

    if tables:
        logger.info("Loading specific tables: %s", ", ".join(tables))
    else:
        logger.info("Loading all available tables (this may take a while)")

    load_info = edxorg_s3_pipeline.run(edxorg_s3_source(tables=tables))
    logger.info("Pipeline completed: %s", load_info)

    return load_info


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    # For testing, only load a small subset of tables
    test_tables = ["auth_user", "student_courseenrollment"]

    logger.info("Running with test tables for faster execution.")
    logger.info("To load all tables, call run_pipeline(tables=None)")

    run_pipeline(tables=test_tables)
