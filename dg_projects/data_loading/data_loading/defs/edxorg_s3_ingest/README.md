# EdX.org S3 Data Ingestion

This directory contains the dlt pipeline for ingesting edX.org database table exports from S3.

## Overview

The edX.org S3 pipeline loads TSV (tab-separated value) files from S3 that are generated by the `edxorg_archive` Dagster asset. These files contain database table exports from MIT's edX.org courses.

**Key Features**:
- **Incremental Loading**: Uses dlt's incremental loading based on file `modification_date`. Only files modified since the last successful run are processed.
- **Prod-Only Filtering**: Only processes files from the `prod` source system, excluding `edge` (staging) data.
- **Asset Dependencies**: Depends on upstream partitioned `edxorg_archive` assets and consolidates all courses into non-partitioned tables.
- **Environment-Aware**: Automatically configures destinations based on `DAGSTER_ENVIRONMENT` (local/qa/production).

### Data Source

**S3 Location**: `s3://ol-data-lake-landing-zone-production/edxorg-raw-data/edxorg/raw_data/db_table/`

**Structure**:
```
db_table/
├── {table_name}/          # e.g., auth_user, student_courseenrollment
│   ├── edge/              # Edge (staging) environment
│   │   └── {course_id}/   # e.g., MITProfessionalX-6.BDX-2015_T3
│   │       └── {hash}.tsv # Actual data file
│   └── prod/              # Production environment
│       └── {course_id}/
│           └── {hash}.tsv
```

### Available Tables

The pipeline can load 40+ database tables including:

**User & Authentication**:
- `auth_user` - User account information
- `auth_userprofile` - User profile data
- `student_anonymoususerid` - Anonymous user IDs

**Enrollment & Progress**:
- `student_courseenrollment` - Course enrollments
- `courseware_studentmodule` - Student module progress
- `student_courseaccessrole` - Course access roles

**Assessments & Submissions**:
- `submissions_submission` - Student submissions
- `submissions_score` - Submission scores
- `assessment_*` - Various assessment tables

**Certificates & Grading**:
- `certificates_generatedcertificate` - Generated certificates
- `grades_persistentcoursegrade` - Course grades
- `grades_persistentsubsectiongrade` - Subsection grades

See `loads.py` for the complete list of 40+ tables.

## Configuration

### AWS Authentication

**This pipeline uses AWS IAM-based authentication** - no explicit credentials needed!

**Local Development**:
- Configure AWS CLI: `aws configure`
- Credentials stored in `~/.aws/credentials`
- dlt automatically uses these credentials via boto3's credential chain

**Production/Kubernetes**:
- Uses IRSA (IAM Roles for Service Accounts)
- IAM role attached to pod provides temporary credentials automatically
- No secret management needed

**Testing your AWS credentials**:
```bash
# Verify you can access the S3 bucket
aws s3 ls s3://ol-data-lake-landing-zone-production/edxorg-raw-data/edxorg/raw_data/db_table/ --recursive | head
```

### Source Configuration

The S3 bucket URL is configured in `.dlt/config.toml`:
```toml
[sources.loads.edxorg_s3_source]
bucket_url = "s3://ol-data-lake-landing-zone-production/edxorg-raw-data/edxorg/raw_data/"
```

### Destinations

The pipeline automatically selects the appropriate destination based on the `DAGSTER_ENVIRONMENT` variable:

**Local Development** (`DAGSTER_ENVIRONMENT=dev` or unset - default):
- Writes to `.dlt/data/` as Parquet files
- No Iceberg overhead for fast iteration
- Query with DuckDB for local testing
- No Glue catalog registration

**QA Environment** (`DAGSTER_ENVIRONMENT=qa`):
- Writes Iceberg tables to `s3://ol-data-lake-raw-qa/edxorg`
- Registers tables in `ol_warehouse_qa_raw` Glue database
- Full Iceberg format with schema evolution support
- Queryable via Athena or Trino

**Production Environment** (`DAGSTER_ENVIRONMENT=production`):
- Writes Iceberg tables to `s3://ol-data-lake-raw-production/edxorg`
- Registers tables in `ol_warehouse_production_raw` Glue database
- Full Iceberg format with schema evolution support
- Queryable via Athena or Trino

**Environment configuration** (`.dlt/config.toml`):
```toml
# Local - Parquet only
[destination.filesystem.local]
bucket_url = "file:///.dlt/data"

# QA - Iceberg + Glue catalog
[destination.filesystem.qa]
bucket_url = "s3://ol-data-lake-raw-qa/edxorg"

[iceberg_catalog.qa]
iceberg_catalog_name = "ol_warehouse_qa_raw"
iceberg_catalog_type = "glue"

# Production - Iceberg + Glue catalog
[destination.filesystem.production]
bucket_url = "s3://ol-data-lake-raw-production/edxorg"

[iceberg_catalog.production]
iceberg_catalog_name = "ol_warehouse_production_raw"
iceberg_catalog_type = "glue"
```

## Dagster Asset Dependencies

The dlt assets created by this pipeline depend on upstream assets from the `edxorg` code location and consolidate their data:

### Upstream Assets (Partitioned)
- **Location**: `edxorg` code location
- **Asset Keys**: `AssetKey(["edxorg", "raw_data", "db_table", {table_name}])`
- **Partitioning**: Multi-partitioned by:
  - `course_id` - Individual course identifier (e.g., `MITProfessionalX-6.BDX-2015_T3`)
  - `source_system` - Either `prod` or `edge`
- **Generated By**: `process_edxorg_archive_bundle` operation in `dg_projects/edxorg/edxorg/assets/edxorg_archive.py`
- **Output**: TSV files written to S3 at `db_table/{table}/{source}/{course}/*.tsv`

### Downstream Assets (Non-Partitioned)
- **Location**: `data_loading` code location
- **Asset Keys**: `AssetKey(["edxorg", "tables", {table_name}])`
- **Table Names**: `raw__edxorg__s3__tables__{table_name}` (e.g., `raw__edxorg__s3__tables__auth_user`)
- **Partitioning**: None - consolidates all upstream partitions
- **Filtering**: Only processes `prod` source (excludes `edge`)
- **Output**: Single table per database table combining all courses

### How It Works

1. **Upstream Generation**: When the `edxorg_archive` asset runs, it exports database tables to S3 as partitioned TSV files (one per course/source combination)

2. **File Glob Filtering**: The dlt source uses glob pattern `db_table/{table}/prod/**/*.tsv` to only read prod files

3. **Dependency Tracking**: Dagster tracks that each consolidated dlt asset depends on its corresponding upstream `db_table` asset

4. **Consolidated Loading**: The dlt pipeline reads all prod TSV files for a table and loads them into a single consolidated table (Parquet or Iceberg)

5. **Incremental Updates**: On subsequent runs, dlt only processes files with `modification_date` newer than the last run

This architecture allows:
- ✅ Lineage tracking from raw exports → consolidated tables
- ✅ Automatic materialization when upstream partitions update
- ✅ Separation of prod vs edge data processing
- ✅ Efficient incremental loading without reprocessing unchanged files

## Local Development

### Run Pipeline Locally

```bash
# Navigate to data_loading directory
cd /path/to/ol-data-platform/dg_projects/data_loading

# Run with local destination (loads all 40+ tables)
uv run python -m data_loading.defs.edxorg_s3_ingest.loads

# Data will be written to .dlt/data/edxorg_*
```

### Load Specific Tables

Edit `loads.py` to specify which tables to load:

```python
if __name__ == "__main__":
    # Load only specific tables
    run_pipeline(tables=["auth_user", "student_courseenrollment"])

    # Or load all tables (warning: slow and large!)
    # run_pipeline(tables=None)
```

### Query Local Data with DuckDB

```bash
# Query auth_user table
duckdb -c "
SELECT * FROM read_parquet('.dlt/data/edxorg_auth_user/*.parquet') LIMIT 10;
"

# Count enrollments
duckdb -c "
SELECT COUNT(*) as enrollment_count
FROM read_parquet('.dlt/data/edxorg_student_courseenrollment/*.parquet');
"

# Join users and enrollments
duckdb -c "
SELECT u.username, COUNT(*) as courses
FROM read_parquet('.dlt/data/edxorg_auth_user/*.parquet') u
JOIN read_parquet('.dlt/data/edxorg_student_courseenrollment/*.parquet') e
  ON u.id = e.user_id
GROUP BY u.username
ORDER BY courses DESC
LIMIT 10;
"
```

## Dagster Integration

The pipeline is automatically discovered by Dagster via the `DltLoadCollectionComponent` defined in `defs.yaml`.

### View Assets

```bash
cd /path/to/ol-data-platform/dg_projects/lakehouse
SKIP_AIRBYTE=1 dg list defs | grep edxorg
```

### Materialize via Dagster

Assets will appear in the Dagster UI under the "ingestion" group. Materialize through the UI or CLI.

## Data Characteristics

### File Format
- **Format**: TSV (tab-separated values)
- **Encoding**: UTF-8
- **Headers**: Yes (first row)
- **Quoting**: Minimal

### Data Volume
- **Files per table**: Varies by course (1 file per course_id)
- **File size**: Typically 1KB to several MB per file
- **Total tables**: 40+ available
- **Update frequency**: Generated by upstream edxorg_archive asset

### Schema
TSV files have varying schemas depending on the table. Common fields include:
- `id` - Primary key (often present)
- `user_id`, `course_id` - Foreign keys
- `created`, `modified` - Timestamps
- Plus table-specific columns

## Performance Considerations

### Memory Usage
- Files are processed in chunks (1000 rows at a time)
- Streaming approach minimizes memory footprint
- Safe to load multiple tables concurrently

### Load Time
- **Single table**: ~30 seconds to several minutes depending on size
- **All tables**: Can take 30+ minutes (not recommended for local dev)
- **Recommendation**: Load only tables you need for testing

### Incremental Loading

**Enabled by default!** The pipeline uses dlt's built-in incremental loading based on file `modification_date`:

- **First run**: Loads all matching TSV files and tracks the latest modification date
- **Subsequent runs**: Only processes files modified since the last successful run
- **State management**: dlt automatically persists incremental state in pipeline storage
- **Performance**: Dramatically reduces load time after initial full load

**How it works**:
1. Each file's `modification_date` from S3 is tracked
2. dlt saves the maximum modification date in pipeline state (`.dlt/<pipeline_name>/` directory)
3. On next run, only files with `modification_date > saved_max_date` are processed
4. The state updates automatically after each successful run

**Manual state reset** (to force full reload):
```bash
# Delete state directory to start fresh
rm -rf .dlt/edxorg_s3_local/
```

**Monitoring incremental loads**:
- Check `_dlt_file_modified` column in loaded data to see file timestamps
- View dlt logs for "incremental" messages showing state management

## Troubleshooting

### AWS Credentials Errors

```
botocore.exceptions.NoCredentialsError: Unable to locate credentials
```

**Solution**: Configure AWS credentials in `.dlt/secrets.toml` or environment variables.

### File Not Found Errors

```
FileNotFoundError: No files found matching pattern
```

**Solution**:
- Verify S3 bucket URL is correct
- Check table name spelling
- Ensure AWS credentials have S3 read permissions

### Memory Errors

```
MemoryError: Unable to allocate array
```

**Solution**:
- Load fewer tables at once
- Reduce chunk size in `read_csv(chunksize=...)`
- Use production environment with more resources

### Empty Results

**Check**:
- S3 path is correct (includes `/edxorg/raw_data/`)
- Table name matches exactly (case-sensitive)
- Files exist for the specified table

## Customization

### Load Additional Tables

To add more tables, update the `all_tables` list in `loads.py`:

```python
all_tables = [
    # ... existing tables ...
    "your_new_table_name",
]
```

### Change File Pattern

To load from a different S3 structure, modify the `file_glob` pattern:

```python
file_glob = f"your/custom/path/{table}/**/*.tsv"
```

### Add Data Transformations

Apply transformations in the `load_table` function:

```python
for record in chunk.to_dict(orient="records"):
    # Transform data
    record["email"] = record.get("email", "").lower()
    record["processed_date"] = datetime.now()

    yield record
```

## References

- [Upstream Asset: edxorg_archive.py](../../edxorg/edxorg/assets/edxorg_archive.py)
- [dlt Filesystem Source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem)
- [dlt CSV Reader](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/basic#read-various-formats)
- [S3 Integration Guide](https://dlthub.com/docs/dlt-ecosystem/destinations/filesystem#s3)

## Related Pipelines

- **Qualtrics Ingestion**: REST API-based ingestion example
- **edxorg_archive (upstream)**: Generates the TSV files this pipeline consumes
