# EdX.org S3 Data Ingestion

This directory contains the dlt pipeline for ingesting edX.org database table exports from S3.

## Overview

The edX.org S3 pipeline loads TSV (tab-separated value) files from S3 that are generated by the `edxorg_archive` Dagster asset. These files contain database table exports from MIT's edX.org courses.

### Data Source

**S3 Location**: `s3://ol-data-lake-landing-zone-production/edxorg-raw-data/edxorg/raw_data/db_table/`

**Structure**:
```
db_table/
├── {table_name}/          # e.g., auth_user, student_courseenrollment
│   ├── edge/              # Edge (staging) environment
│   │   └── {course_id}/   # e.g., MITProfessionalX-6.BDX-2015_T3
│   │       └── {hash}.tsv # Actual data file
│   └── prod/              # Production environment
│       └── {course_id}/
│           └── {hash}.tsv
```

### Available Tables

The pipeline can load 40+ database tables including:

**User & Authentication**:
- `auth_user` - User account information
- `auth_userprofile` - User profile data
- `student_anonymoususerid` - Anonymous user IDs

**Enrollment & Progress**:
- `student_courseenrollment` - Course enrollments
- `courseware_studentmodule` - Student module progress
- `student_courseaccessrole` - Course access roles

**Assessments & Submissions**:
- `submissions_submission` - Student submissions
- `submissions_score` - Submission scores
- `assessment_*` - Various assessment tables

**Certificates & Grading**:
- `certificates_generatedcertificate` - Generated certificates
- `grades_persistentcoursegrade` - Course grades
- `grades_persistentsubsectiongrade` - Subsection grades

See `loads.py` for the complete list of 40+ tables.

## Configuration

### Credentials

Create or update `.dlt/secrets.toml` in the lakehouse root:

```toml
[sources.edxorg_s3]
bucket_url = "s3://ol-data-lake-landing-zone-production/edxorg-raw-data/edxorg/raw_data/"
aws_access_key_id = "YOUR_AWS_ACCESS_KEY"
aws_secret_access_key = "YOUR_AWS_SECRET_KEY"
```

For local development with AWS CLI configured:
```bash
# Use AWS CLI credentials
export AWS_PROFILE=your-profile
# Or set credentials directly
export EDXORG_S3__AWS_ACCESS_KEY_ID="..."
export EDXORG_S3__AWS_SECRET_ACCESS_KEY="..."
```

### Destinations

The pipeline supports two destinations configured via `DLT_DESTINATION_ENV`:

**Local Development** (default):
- Writes to `.dlt/data/` as Parquet files
- Fast iteration for testing
- Query with DuckDB

**Production**:
- Writes to S3 as Parquet files (configured in `.dlt/config.toml`)
- Registers tables in AWS Glue catalog

## Local Development

### Run Pipeline Locally

```bash
# Navigate to lakehouse directory
cd /path/to/ol-data-platform/dg_projects/lakehouse

# Run with local destination (loads sample tables)
python -m lakehouse.defs.edxorg_s3_ingestion.loads

# Data will be written to .dlt/data/edxorg_*
```

### Load Specific Tables

Edit `loads.py` to specify which tables to load:

```python
if __name__ == "__main__":
    # Load only specific tables
    run_pipeline(tables=["auth_user", "student_courseenrollment"])

    # Or load all tables (warning: slow and large!)
    # run_pipeline(tables=None)
```

### Query Local Data with DuckDB

```bash
# Query auth_user table
duckdb -c "
SELECT * FROM read_parquet('.dlt/data/edxorg_auth_user/*.parquet') LIMIT 10;
"

# Count enrollments
duckdb -c "
SELECT COUNT(*) as enrollment_count
FROM read_parquet('.dlt/data/edxorg_student_courseenrollment/*.parquet');
"

# Join users and enrollments
duckdb -c "
SELECT u.username, COUNT(*) as courses
FROM read_parquet('.dlt/data/edxorg_auth_user/*.parquet') u
JOIN read_parquet('.dlt/data/edxorg_student_courseenrollment/*.parquet') e
  ON u.id = e.user_id
GROUP BY u.username
ORDER BY courses DESC
LIMIT 10;
"
```

## Dagster Integration

The pipeline is automatically discovered by Dagster via the `DltLoadCollectionComponent` defined in `defs.yaml`.

### View Assets

```bash
cd /path/to/ol-data-platform/dg_projects/lakehouse
SKIP_AIRBYTE=1 dg list defs | grep edxorg
```

### Materialize via Dagster

Assets will appear in the Dagster UI under the "ingestion" group. Materialize through the UI or CLI.

## Data Characteristics

### File Format
- **Format**: TSV (tab-separated values)
- **Encoding**: UTF-8
- **Headers**: Yes (first row)
- **Quoting**: Minimal

### Data Volume
- **Files per table**: Varies by course (1 file per course_id)
- **File size**: Typically 1KB to several MB per file
- **Total tables**: 40+ available
- **Update frequency**: Generated by upstream edxorg_archive asset

### Schema
TSV files have varying schemas depending on the table. Common fields include:
- `id` - Primary key (often present)
- `user_id`, `course_id` - Foreign keys
- `created`, `modified` - Timestamps
- Plus table-specific columns

## Performance Considerations

### Memory Usage
- Files are processed in chunks (1000 rows at a time)
- Streaming approach minimizes memory footprint
- Safe to load multiple tables concurrently

### Load Time
- **Single table**: ~30 seconds to several minutes depending on size
- **All tables**: Can take 30+ minutes (not recommended for local dev)
- **Recommendation**: Load only tables you need for testing

### Incremental Loading
Current implementation does **full refresh** each run. To enable incremental loading:

1. Add `last_modified` tracking to resources
2. Use dlt's incremental loading features
3. Track processed files in dlt state

## Troubleshooting

### AWS Credentials Errors

```
botocore.exceptions.NoCredentialsError: Unable to locate credentials
```

**Solution**: Configure AWS credentials in `.dlt/secrets.toml` or environment variables.

### File Not Found Errors

```
FileNotFoundError: No files found matching pattern
```

**Solution**:
- Verify S3 bucket URL is correct
- Check table name spelling
- Ensure AWS credentials have S3 read permissions

### Memory Errors

```
MemoryError: Unable to allocate array
```

**Solution**:
- Load fewer tables at once
- Reduce chunk size in `read_csv(chunksize=...)`
- Use production environment with more resources

### Empty Results

**Check**:
- S3 path is correct (includes `/edxorg/raw_data/`)
- Table name matches exactly (case-sensitive)
- Files exist for the specified table

## Customization

### Load Additional Tables

To add more tables, update the `all_tables` list in `loads.py`:

```python
all_tables = [
    # ... existing tables ...
    "your_new_table_name",
]
```

### Change File Pattern

To load from a different S3 structure, modify the `file_glob` pattern:

```python
file_glob = f"your/custom/path/{table}/**/*.tsv"
```

### Add Data Transformations

Apply transformations in the `load_table` function:

```python
for record in chunk.to_dict(orient="records"):
    # Transform data
    record["email"] = record.get("email", "").lower()
    record["processed_date"] = datetime.now()

    yield record
```

## References

- [Upstream Asset: edxorg_archive.py](../../edxorg/edxorg/assets/edxorg_archive.py)
- [dlt Filesystem Source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem)
- [dlt CSV Reader](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/basic#read-various-formats)
- [S3 Integration Guide](https://dlthub.com/docs/dlt-ecosystem/destinations/filesystem#s3)

## Related Pipelines

- **Qualtrics Ingestion**: REST API-based ingestion example
- **edxorg_archive (upstream)**: Generates the TSV files this pipeline consumes
