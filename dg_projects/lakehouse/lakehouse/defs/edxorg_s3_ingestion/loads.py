"""
EdX.org S3 CSV/TSV data ingestion via dlt.

This module loads TSV files from S3 that are generated by the edxorg_archive.py
asset. These files contain database table exports from edX.org courses.

S3 Structure:
s3://ol-data-lake-landing-zone-production/edxorg-raw-data/edxorg/raw_data/db_table/{table_name}/{source_system}/{course_id}/{hash}.tsv

Tables include:
- auth_user, auth_userprofile
- student_courseenrollment, courseware_studentmodule
- certificates_generatedcertificate
- submissions_*, assessment_*
- And many more...

Usage (standalone):
    # Local development (default - writes to .dlt/data/)
    python -m lakehouse.defs.edxorg_s3_ingestion.loads

    # Production (writes to S3)
    DLT_DESTINATION_ENV=production python -m lakehouse.defs.edxorg_s3_ingestion.loads
"""

import os
from collections.abc import Iterator

import dlt
from dlt.sources.filesystem import filesystem, read_csv


@dlt.source
def edxorg_s3_source(
    bucket_url: str = dlt.config.value,
    aws_access_key_id: str = dlt.secrets.value,
    aws_secret_access_key: str = dlt.secrets.value,
    tables: list[str] | None = None,
):
    """
    Load edxorg CSV/TSV data from S3.

    Args:
        bucket_url: S3 bucket URL (e.g., s3://bucket-name/path/prefix)
        aws_access_key_id: AWS access key
        aws_secret_access_key: AWS secret key
        tables: Optional list of specific tables to load. If None, loads all tables.
                Examples: ["auth_user", "student_courseenrollment"]

    Yields:
        dlt resources for each table type found in S3
    """

    # Available table names from edxorg_archive.py
    all_tables = [
        "assessment_assessment",
        "assessment_assessmentfeedback",
        "assessment_assessmentfeedback_assessments",
        "assessment_assessmentfeedback_options",
        "assessment_assessmentfeedbackoption",
        "assessment_assessmentpart",
        "assessment_criterion",
        "assessment_criterionoption",
        "assessment_peerworkflow",
        "assessment_peerworkflowitem",
        "assessment_rubric",
        "assessment_studenttrainingworkflow",
        "assessment_studenttrainingworkflowitem",
        "assessment_trainingexample",
        "assessment_trainingexample_options_selected",
        "auth_user",
        "auth_userprofile",
        "certificates_generatedcertificate",
        "course_groups_cohortmembership",
        "courseware_studentmodule",
        "credit_crediteligibility",
        "django_comment_client_role_users",
        "grades_persistentcoursegrade",
        "grades_persistentsubsectiongrade",
        "student_anonymoususerid",
        "student_courseaccessrole",
        "student_courseenrollment",
        "student_languageproficiency",
        "submissions_score",
        "submissions_scoresummary",
        "submissions_studentitem",
        "submissions_submission",
        "teams",
        "teams_membership",
        "user_api_usercoursetag",
        "user_id_map",
        "wiki_article",
        "wiki_articlerevision",
        "workflow_assessmentworkflow",
        "workflow_assessmentworkflowstep",
    ]

    # Filter to requested tables if specified
    tables_to_load = tables if tables is not None else all_tables

    for table_name in tables_to_load:
        # Create a resource for each table
        @dlt.resource(
            name=f"edxorg_{table_name}",
            write_disposition="merge",
            primary_key=None,  # TSV files don't have consistent primary keys
        )
        def load_table(table=table_name) -> Iterator[dict]:
            """Load TSV files for a specific table from S3."""

            # Pattern to match all TSV files for this table
            # Matches: db_table/{table_name}/*/*.tsv (source_system/course_id/*.tsv)
            file_glob = f"db_table/{table}/**/*.tsv"

            # Use dlt's filesystem source to discover files
            files = filesystem(
                bucket_url=bucket_url,
                file_glob=file_glob,
                credentials={
                    "aws_access_key_id": aws_access_key_id,
                    "aws_secret_access_key": aws_secret_access_key,
                },
            )

            # Read and yield data from each TSV file
            for file_item in files:
                # Read TSV with appropriate settings
                csv_reader = read_csv(
                    file_item,
                    delimiter="\t",  # TSV files use tabs
                    chunksize=1000,  # Process in chunks for memory efficiency
                )

                for chunk in csv_reader:
                    # Convert chunk to records and yield
                    for record in chunk.to_dict(orient="records"):
                        # Add metadata about source file
                        record["_dlt_file_path"] = file_item["file_name"]
                        record["_dlt_load_timestamp"] = dlt.current.load_id()
                        yield record

        yield load_table


# Determine destination environment (local or production)
destination_env = os.getenv("DLT_DESTINATION_ENV", "local")

# Create pipeline with environment-specific configuration
edxorg_s3_pipeline = dlt.pipeline(
    pipeline_name="edxorg_s3",
    destination="filesystem",
    dataset_name=f"edxorg_s3_{destination_env}",
    progress="log",
)


def run_pipeline(tables: list[str] | None = None):
    """
    Execute the edxorg S3 pipeline.

    Args:
        tables: Optional list of specific tables to load.
                If None, loads all tables (can be slow!).
                Example: ["auth_user", "student_courseenrollment"]
    """
    print(f"Running edxorg S3 pipeline with destination: {destination_env}")

    if tables:
        print(f"Loading specific tables: {', '.join(tables)}")
    else:
        print("Loading all available tables (this may take a while)")

    load_info = edxorg_s3_pipeline.run(edxorg_s3_source(tables=tables))
    print(load_info)

    return load_info


if __name__ == "__main__":
    # For testing, only load a small subset of tables
    test_tables = ["auth_user", "student_courseenrollment"]

    print("Running with test tables for faster execution.")
    print("To load all tables, call run_pipeline(tables=None)")

    run_pipeline(tables=test_tables)
